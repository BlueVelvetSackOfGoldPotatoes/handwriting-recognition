{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d88a49ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted labels\n",
      "P168-Fg016-R-C01-R01-binarized.jpg\n",
      "1/1 [==============================] - 3s 3s/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "[' והוה    י', 'פךרסדאשודב', 'הגבאחםל', 'שוחבגסנליש', 'ןעוטדאעחטהושע', 'סטטגץןןנר', 'גהנלוס הנשה', 'עלחקור וורג', 'גץ םל  ']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "\n",
    "# Standard vocab to ensure correct encoding\n",
    "vocab = [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\", \"o\", \"p\", \"q\", \"r\", \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\", \"+\", \" \"]\n",
    "\n",
    "# Batch size for training and validation\n",
    "batch_size = 1\n",
    "\n",
    "# Desired image dimensions\n",
    "img_width = 499\n",
    "img_height = 60\n",
    "\n",
    "# Factor by which the image is going to be downsampled\n",
    "# by the convolutional blocks. We will be using two\n",
    "# convolution blocks and each block will have\n",
    "# a pooling layer which downsample the features by a factor of 2.\n",
    "# Hence total downsampling factor would be 4.\n",
    "downsample_factor = 4\n",
    "\n",
    "# Preprocessing ----------------------------------------------------------\n",
    "# Mapping characters to integers\n",
    "char_to_num = layers.StringLookup(\n",
    "    vocabulary=vocab, mask_token=None\n",
    ")\n",
    "\n",
    "# Mapping integers back to original characters\n",
    "num_to_char = layers.StringLookup(\n",
    "    vocabulary=char_to_num.get_vocabulary(), mask_token=None, invert=True\n",
    ")\n",
    "\n",
    "\n",
    "def encode_single_sample(img_path, label):\n",
    "    # 1. Read image\n",
    "    img = tf.io.read_file(img_path)\n",
    "    # 2. Decode and convert to grayscale\n",
    "    img = tf.io.decode_png(img, channels=1)\n",
    "    # 3. Convert to float32 in [0, 1] range\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    # 4. Resize to the desired size\n",
    "    img = tf.image.resize(img, [img_height, img_width])\n",
    "    # 5. Transpose the image because we want the time\n",
    "    # dimension to correspond to the width of the image.\n",
    "    img = tf.transpose(img, perm=[1, 0, 2])\n",
    "    # 6. Map the characters in label to numbers\n",
    "    label = char_to_num(tf.strings.unicode_split(label, input_encoding=\"UTF-8\"))\n",
    "    # 7. Return a dict as our model is expecting two inputs\n",
    "    return {\"image\": img, \"label\": label}\n",
    "\n",
    "model = keras.models.load_model('trained_model')\n",
    "# as max length is based on labels (which in this case are not relevant) we set it to a high nr so predictions can be made for lines of all lengths\n",
    "max_length = 50\n",
    "# Inference ----------------------------------\n",
    "# Get the prediction model by extracting layers till the output layer\n",
    "prediction_model = keras.models.Model(\n",
    "    model.get_layer(name=\"image\").input, model.get_layer(name=\"dense2\").output\n",
    ")\n",
    "#prediction_model.summary()\n",
    "\n",
    "# A utility function to decode the output of the network\n",
    "def decode_batch_predictions(pred):\n",
    "    input_len = np.ones(pred.shape[0]) * pred.shape[1]\n",
    "    # Use greedy search. For complex tasks, you can use beam search\n",
    "    results = keras.backend.ctc_decode(pred, input_length=input_len, greedy=True)[0][0][\n",
    "        :, :max_length\n",
    "    ]\n",
    "    # Iterate over the results and get back the text\n",
    "    output_text = []\n",
    "    for res in results:\n",
    "        res = tf.strings.reduce_join(num_to_char(res)).numpy().decode(\"utf-8\")\n",
    "        output_text.append(res)\n",
    "    return output_text\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "rootdir = './Preproc_Outputs'\n",
    "rootdir2 = 'results'\n",
    "\n",
    "os.makedirs(rootdir2, exist_ok=True)\n",
    "\n",
    "save_path = os.getcwd()\n",
    "#fin_out = (\"Final_output\")\n",
    "fin_out_complete = os.path.join(save_path, rootdir2)  \n",
    "#print(fin_out_complete)\n",
    "\n",
    "\n",
    "for root, dirs, files in os.walk(rootdir):\n",
    "    temp_lines = []\n",
    "    if \"combined\" in dirs:\n",
    "        dirs.remove(\"combined\")\n",
    "    if \"Cropped\" in dirs:\n",
    "        folderName = os.path.split(root)[1]\n",
    "        #print(folderName)\n",
    "        data_dir = Path(f\"{root}/Cropped/\")\n",
    "        #print(data_dir)\n",
    "        l = str(data_dir)\n",
    "        images = sorted(list(map(str, list(data_dir.glob(\"*.jpg\")))))\n",
    "        labels = [img.split(os.path.sep)[-1].split(\".jpg\")[0] for img in images]\n",
    "        #print(images)\n",
    "        test_dataset = tf.data.Dataset.from_tensor_slices((images, labels))\n",
    "        test_dataset = (test_dataset.map(encode_single_sample).batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE))\n",
    "        print(\"Predicted labels\")\n",
    "        print(folderName)\n",
    "        #print(test_dataset)\n",
    "        \n",
    "        for batch in test_dataset:\n",
    "            batch_images = batch[\"image\"]\n",
    "            batch_labels = batch[\"label\"]\n",
    "            preds = prediction_model.predict(batch_images)\n",
    "            pred_texts = decode_batch_predictions(preds)\n",
    "            new_pred = list()\n",
    "            for i in range(len(pred_texts)):\n",
    "                newstr = pred_texts[i].replace(\"[UNK]\", \"\")\n",
    "                new_pred += [newstr]\n",
    "                #print(new_pred)\n",
    "            \n",
    "            for i in range(len(new_pred)):\n",
    "                line = new_pred[i].replace(\"a\", \"א\")\n",
    "                line = line.replace(\"b\", \"ע\")\n",
    "                line = line.replace(\"c\", \"ב\")\n",
    "                line = line.replace(\"d\", \"ד\")\n",
    "                line = line.replace(\"e\", \"ג\")\n",
    "                line = line.replace(\"g\", \"ח\")\n",
    "                line = line.replace(\"f\", \"ה\")\n",
    "                line = line.replace(\"i\", \"ך\")\n",
    "                line = line.replace(\"h\", \"כ\")\n",
    "                line = line.replace(\"j\", \"ל\")\n",
    "                line = line.replace(\"l\", \"מ\")\n",
    "                line = line.replace(\"k\", \"ם\")\n",
    "                line = line.replace(\"m\", \"ן\")\n",
    "                line = line.replace(\"n\", \"נ\")\n",
    "                line = line.replace(\"p\", \"ף\")\n",
    "                line = line.replace(\"o\", \"פ\")\n",
    "                line = line.replace(\"q\", \"ק\")\n",
    "                line = line.replace(\"r\", \"ר\")\n",
    "                line = line.replace(\"s\", \"ס\")\n",
    "                line = line.replace(\"t\", \"ש\")\n",
    "                line = line.replace(\"u\", \"ת\")\n",
    "                line = line.replace(\"v\", \"ט\")\n",
    "                line = line.replace(\"w\", \"ץ\")\n",
    "                line = line.replace(\"x\", \"צ\")\n",
    "                line = line.replace(\"y\", \"ו\")\n",
    "                line = line.replace(\"z\", \"י\")\n",
    "                line = line[::-1]\n",
    "                temp_lines += [line]\n",
    "                #print(temp_lines)\n",
    "        \n",
    "                max_len = max([len(K) for K in temp_lines])\n",
    "                name_of_file = str(folderName)\n",
    "                completeName = os.path.join(fin_out_complete, name_of_file+\".txt\")         \n",
    "                fo = open(completeName, 'w', encoding='utf-8')\n",
    "                for z in temp_lines:\n",
    "                    # each line is padded with the maximum length\n",
    "                    fo.write(z.rjust(max_len) + \"\\n\")\n",
    "                fo.close()\n",
    "        print(temp_lines)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
